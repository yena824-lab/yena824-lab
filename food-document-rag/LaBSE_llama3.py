from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
import torch
import pandas as pd
from tqdm import tqdm
import os
import numpy as np
import faiss
import random
import accelerate
import pickle
import json
import gc

os.environ["CUDA_VISIBLE_DEVICES"] = "0, 3"

# ì—‘ì…€ ì½ê¸°
file_path = "/home/yena/Food_RAG/ì‹í’ˆì•ˆì „ì •ë³´DB-url ì¶”ê°€(2014~2023).xls"
df = pd.read_excel(file_path, sheet_name="2023", usecols=["ì œëª©", "ë‚´ìš©"])
df["ì œëª©_ë‚´ìš©"] = df["ì œëª©"] + " " + df["ë‚´ìš©"]
data = df["ì œëª©_ë‚´ìš©"].to_list()

# ë¬¸ì¥ ì„ë² ë”©ìš© LaBSE
labse_tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/LaBSE")
labse_model = AutoModel.from_pretrained("sentence-transformers/LaBSE")

def mean_pooling(model_output, attention_mask):
    """Mean Pooling ì ìš© (íŒ¨ë”© í† í° ì œì™¸)"""
    token_embeddings = model_output.last_hidden_state
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())

    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)
    sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)  # íŒ¨ë”© ë¶€ë¶„ ì œì™¸í•˜ê³  í‰ê·  ê³„ì‚°
    return sum_embeddings / sum_mask

def embed_texts(text_list):
    """LaBSE ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ Mean Pooling ì ìš©í•œ ë¬¸ì¥ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    embeddings_list = []
    
    for text in tqdm(text_list):
        inputs = labse_tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors="pt")
        
        with torch.no_grad():
            outputs = labse_model(**inputs)
        
        sentence_embedding = mean_pooling(outputs, inputs['attention_mask'])
        embeddings_list.append(sentence_embedding)
    
    return torch.stack(embeddings_list).squeeze(1)

def embed_texts_CLS(text_list, model, tokenizer):
    """CLS í† í°ì„ ì‚¬ìš©í•œ ë¬¸ì¥ ì„ë² ë”© ìƒì„± (ì§„í–‰ë¥  í‘œì‹œ)"""
    embeddings_list = []
    
    for text in text_list:
        inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        cls_embedding = outputs.last_hidden_state[:, 0, :]  # CLS í† í°ì˜ ì„ë² ë”©
        embeddings_list.append(cls_embedding)
    
    return torch.stack(embeddings_list).squeeze(1)

def prompting_answer(question, data):
    messages = [
        {'role': 'system', 'content': 'ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” í•œêµ­ì–´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.'},
        {'role': 'user', 'content': 'ë‹¹ì‹ ì˜ ì—­í• ì€ ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ìì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì •ë³´ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì£¼ì–´ì§‘ë‹ˆë‹¤. ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì´ ìš”êµ¬ë©ë‹ˆë‹¤.'},
        {"role": "assistant", "content": "ë¬¼ë¡ ì…ë‹ˆë‹¤! ì§ˆë¬¸ì´ ë¬´ì—‡ì¸ì§€ ì•Œë ¤ì£¼ì‹œë©´, ìµœì„ ì„ ë‹¤í•´ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."},
        {"role": "user", "content": f'''
            ì •ë³´ : {data}
            ì§ˆë¬¸ : {question}
            (ë¹„ê³ , ë‹¤ìŒì˜ ì¡°ê±´ë“¤ì„ ì¶©ì¡±í•˜ë„ë¡ ë‹µë³€í•˜ì„¸ìš”.
            - ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±
            - êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ ì œê³µ
            - ì •ë‹µì´ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°, í•œ ê°œë§Œ ì„ íƒí•˜ê³  ì„ íƒ ì´ìœ ë¥¼ ê°„ëµíˆ í¬í•¨)
            '''},
        {"role": "assistant", "content": 'ì •ë‹µ:'}
    ]
    return messages

# embeddings = embed_texts(data)
# with open("/home/yena/Food_RAG/labse_embeddings.pkl", "wb") as f:
#     pickle.dump(embeddings, f)

file_path = "/home/yena/Food_RAG/labse_embeddings.pkl"
with open(file_path, 'rb') as file:  # 'rb'ëŠ” ë°”ì´ë„ˆë¦¬ ì½ê¸° ëª¨ë“œ
    embeddings = pickle.load(file)
embeddings

# 1. ì§ˆì˜ ë¦¬ìŠ¤íŠ¸
query = [
    "Waitrose 8 Red Onion Bhajis with a Date and Tamarind Dip ì œí’ˆì´ ì™œ íšŒìˆ˜ë˜ì—ˆë‚˜?",
    "Hassui Kamaboko Co.,Ltd.ì—ì„œ íšŒìˆ˜í•œ ì œí’ˆì˜ íšŒìˆ˜ ì‚¬ìœ ëŠ”?",
    "í”„ë‘ìŠ¤ Ducourauì‚¬ì˜ êµ´ì´ íšŒìˆ˜ëœ ì´ìœ ëŠ”?",
    "ë§ë ˆì´ì‹œì•„ íŠ¸ë ê°€ëˆ„ì£¼ì—ì„œ ë°œìƒí•œ ì‹ì¤‘ë… ì‚¬ë§ ì›ì¸ì€?",
    "Arauco ì˜¬ë¦¬ë¸Œìœ ê°€ íŒë§¤ê¸ˆì§€ëœ ì´ìœ ëŠ”?",
    "ë²¨ê¸°ì—ì—ì„œ Isla DÃ©lice ì‹ìœ¡ê°€ê³µí’ˆì´ íšŒìˆ˜ëœ ì´ìœ ëŠ”?",
    "ì˜êµ­ Country Kitchenì—ì„œ íšŒìˆ˜í•œ ë¨¸í•€ì˜ ì œí’ˆëª…ì€?",
    "Le Duo des Gors ì¹˜ì¦ˆê°€ íšŒìˆ˜ëœ ì´ìœ ëŠ”?",
    "ë‰´ì§ˆëœë“œì—ì„œ Value brand íƒ„ì‚°ìŒë£Œê°€ íšŒìˆ˜ëœ ì´ìœ ëŠ”?",
    "ë¯¸êµ­ì—ì„œ TGD Cuts, LLCê°€ íšŒìˆ˜í•œ ê³¼ì¼ì˜ ì˜¤ì—¼ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ë³‘ì›ê· ì€?",
    "FishMeatz LLPê°€ ë²Œê¸ˆí˜•ì„ ë°›ì€ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?",
    "Rude Health Organic Coconut Drinkê°€ íšŒìˆ˜ëœ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?",
    "New Roots Herbalì˜ ì•„ìŠˆì™€ê°„ë‹¤ ì œí’ˆì´ íšŒìˆ˜ëœ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?",
    "ë²¨ê¸°ì—ì—ì„œ BERGERONNETTE PÃ©rail du FÃ©dou ì¹˜ì¦ˆê°€ íšŒìˆ˜ëœ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?",
    "ë£¨ìƒ¤ì˜¤ì ì‹í’ˆ ìœ í•œê³µì‚¬ì˜ ì˜¤í–¥ ë‹­ë‚ ê°œê°€ ë¶€ì í•© íŒì •ì„ ë°›ì€ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?",
    "GDE Grocery Delivery E-Services Canada Inc.ê°€ íšŒìˆ˜í•œ ë‹­ê³ ê¸° ì œí’ˆì˜ ìœ í†µì§€ì—­ì€ ì–´ë””ì¸ê°€?",
    "ì´íƒˆë¦¬ì•„ì—ì„œ ìˆ˜ì¶œí•œ ì‹ ì„  ì—¬ë¦„ ì†¡ë¡œë²„ì„¯ì˜ ì¹´ë“œë®´ í•¨ëŸ‰ì€ ì–¼ë§ˆì¸ê°€?",
    "Mrs Kirkham ì¹˜ì¦ˆê°€ íšŒìˆ˜ëœ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?",
    "ì½”ìŠ¤íƒ€ë¦¬ì¹´ ë‚´ ë¦¬ìŠ¤í…Œë¦¬ì•„ì¦ ê°ì—¼ ì‚¬ë¡€ëŠ” ì£¼ë¡œ ì–´ë–¤ ì‹í’ˆê³¼ ê´€ë ¨ì´ ìˆëŠ”ê°€?",
    "ì“°ì´¨ ì´¨ë¼ì˜¤ë¼ì˜¤ ì‹í’ˆ ê³¼í•™ê¸°ìˆ ìœ í•œê³µì‚¬ì˜ ì‹ìš© ì‹ë¬¼ í˜¼í•©ìœ ì—ì„œ ê²€ì¶œëœ ë¶€ì í•© ë¬¼ì§ˆì€ ë¬´ì—‡ì¸ê°€?",
    "ë² íŠ¸ë‚¨ í•˜ë…¸ì´ ì‹œì¥ê´€ë¦¬êµ­ì€ ì–´ë–¤ ë¶ˆë²• í–‰ìœ„ë¥¼ ì ë°œí–ˆëŠ”ê°€?",
    "ì¼ë³¸ ë†ë¦¼ìˆ˜ì‚°ì„±ì´ ì¹ ë ˆì‚° ê°€ê¸ˆìœ¡ ë“±ì˜ ìˆ˜ì…ì¤‘ì§€ ì¡°ì¹˜ë¥¼ í•´ì œí•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?",
    "ìµœê·¼ ì„¸ê³„ë™ë¬¼ë³´ê±´ê¸°êµ¬(WOAH)ê°€ ë³´ê³ í•œ ê³ ë³‘ì›ì„± ì¡°ë¥˜ì¸í”Œë£¨ì—”ì(H5N1) ë°œìƒ í˜„í™©ì€ ì–´ë– í•œê°€?",
    "ì¹ ë ˆì—ì„œ ë³´ê³ ëœ ì¡°ë¥˜ì¸í”Œë£¨ì—”ì A(H5) ì¸ì²´ ê°ì—¼ ì‚¬ë¡€ì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€?",
    "ìŠ¤ìœ„ìŠ¤ ì—°ë°©í‰ì˜íšŒëŠ” í™”í•™ë¬¼ì§ˆ ë° íê¸°ë¬¼ í˜‘ì•½ ê°•í™”ì™€ ê´€ë ¨í•˜ì—¬ ì–´ë–¤ ì¡°ì¹˜ë¥¼ ì·¨í•˜ê³  ìˆëŠ”ê°€?",
    "ë²¨ê¸°ì— ì—°ë°©ë³´ê±´ë¶€ëŠ” ì•„ìŠ¤íŒŒíƒì˜ 1ì¼í—ˆìš©ì„­ì·¨ëŸ‰(ADI)ì„ ë³€ê²½í•˜ì§€ ì•Šì€ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?",
    "ë¯¸êµ­ ì—°êµ¬ì§„ì˜ ì—°êµ¬ì— ë”°ë¥´ë©´, ì¹´ë“œë®´ ì‹ì´ ë…¸ì¶œì´ ê°€ì¥ ë†’ì€ ì—°ë ¹ëŒ€ì™€ ì£¼ìš” ë…¸ì¶œ ì‹í’ˆì€ ë¬´ì—‡ì¸ê°€?",
    "ì¼ë³¸ ìœ í•œíšŒì‚¬ ë°¸ëŸ°ìŠ¤ê°€ ë§ˆë“¤ë Œ ì œí’ˆì„ íšŒìˆ˜í•œ ì´ìœ ì™€ í•´ë‹¹ ì œí’ˆì˜ íŒë§¤ ì •ë³´ëŠ” ë¬´ì—‡ì¸ê°€?",
    "ì¤‘êµ­ í•´ê´€ì´ì„œì™€ ë†ì—…ë†ì´Œë¶€ëŠ” í„°í‚¤ì—ì„œ ë°œìƒí•œ ê°€ì„±ìš°ì—­ì˜ ìœ ì…ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì–´ë–¤ ì¡°ì¹˜ë¥¼ ì‹œí–‰í–ˆëŠ”ê°€?",
    "ëŒ€ë§Œ ì‹ ë² ì´ì‹œì—ì„œ ì ë°œëœ ê°€ì§œ ì–‘ê³ ê¸° íŒë§¤ ì‚¬ê±´ì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€?",
    "Springbank Cheese Co.ì™€ Le Grand Fromageì—ì„œ íšŒìˆ˜ëœ ì¹˜ì¦ˆ ì œí’ˆì€ ë¬´ì—‡ì´ë©°, íšŒìˆ˜ ì‚¬ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
    "ìŠ¤í˜ì¸ ì‹í’ˆì•ˆì „ì˜ì–‘ì²­(AESAN)ì€ ì•„ì¼ëœë“œì‚° ììˆ™ ëƒ‰ë™ê²Œì— ëŒ€í•œ ê²½ê³ ë¥¼ ì™œ ì² íšŒí–ˆë‚˜ìš”?",
    "ì˜êµ­ í™˜ê²½ì‹í’ˆë†ì´Œë¶€(Defra)ê°€ ì•ˆë‚´í•œ êµ­ê²½ ëª©í‘œìš´ì˜ëª¨ë¸(Border Target Operating Model)ì€ ë¬´ì—‡ì¸ê°€?",
    "ë‚˜ì´ì§€ë¦¬ì•„ì‚° íˆë¹„ìŠ¤ì»¤ìŠ¤ ê½ƒì—ì„œ ê²€ì¶œëœ ë¯¸ìŠ¹ì¸ ë¬¼ì§ˆì€ ë¬´ì—‡ì¸ê°€?",
    "ì¼ë³¸ì—ì„œ íšŒìˆ˜ëœ 'íŒ”ë„ ê¼¬ê¼¬ë©´'ì˜ íšŒìˆ˜ ì‚¬ìœ ëŠ” ë¬´ì—‡ì¸ê°€?",
    "ëŒ€ë§Œ ì‹ì•½ì„œê°€ ì¼ë³¸ì‚° ìˆ˜ì…ì‹í’ˆì˜ ë°©ì‚¬ëŠ¥ ê²€ì‚¬ë¥¼ ì¤‘ë‹¨í•œ í’ˆëª©ì€ ë¬´ì—‡ì¸ê°€?",
    "ë„¤íŒ”ì—ì„œ ë³´ê³ ëœ H5N2 ê³ ë³‘ì›ì„± ì¡°ë¥˜ ì¸í”Œë£¨ì—”ìì˜ ë°œìƒ ê·œëª¨ëŠ” ì–´ë–»ê²Œ ë˜ëŠ”ê°€?",
    "ë¯¸êµ­ ì‹í’ˆì˜ì•½í’ˆì²­ì´ 'PrimeZen Black 6000' ì œí’ˆì— ëŒ€í•´ ê²½ê³ í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?",
    "ë…ì¼ CVUAê°€ ì„œì–‘ ì†¡ë¡œë²„ì„¯ì„ í¬í•¨í•œ ì œí’ˆì—ì„œ ì§‘ì¤‘ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?",
    "ë¯¸êµ­ ì‹í’ˆì•ˆì „ê²€ì‚¬êµ­ì´ ê³µì¤‘ë³´ê±´ê²½ë³´ë¥¼ ë°œë ¹í•œ ëƒ‰ë™ ë‹­ê³ ê¸° ì œí’ˆì˜ ì œì¡°ì‚¬ëŠ” ì–´ë””ì¸ê°€?"
]

print("ì§ˆë¬¸ ê°œìˆ˜:", len(query))  # í™•ì¸ìš© ì¶œë ¥

# 2. ì„ë² ë”© ë¶ˆëŸ¬ì˜¤ê¸°
embeddings = torch.load("labse_embeddings.pt")  # torch tensorë¡œ ì €ì¥ëœ íŒŒì¼
embeddings = np.array(embeddings, dtype=np.float32)

# 3. FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì„ë² ë”© ì¶”ê°€
embedding_dim = embeddings.shape[1]
index = faiss.IndexFlatIP(embedding_dim)
faiss.normalize_L2(embeddings)
index.add(embeddings)

def prompting_answer(question, data):
    messages = [
        {'role': 'system', 'content': 'ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” í•œêµ­ì–´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.'},
        {'role': 'user', 'content': 'ë‹¹ì‹ ì˜ ì—­í• ì€ ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ìì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì •ë³´ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì£¼ì–´ì§‘ë‹ˆë‹¤. ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì´ ìš”êµ¬ë©ë‹ˆë‹¤.'},
        {"role": "assistant", "content": "ë¬¼ë¡ ì…ë‹ˆë‹¤! ì§ˆë¬¸ì´ ë¬´ì—‡ì¸ì§€ ì•Œë ¤ì£¼ì‹œë©´, ìµœì„ ì„ ë‹¤í•´ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."},
        {"role": "user", "content": f'''
            ì •ë³´ : {data}
            ì§ˆë¬¸ : {question}
            (ë¹„ê³ , ë‹¤ìŒì˜ ì¡°ê±´ë“¤ì„ ì¶©ì¡±í•˜ë„ë¡ ë‹µë³€í•˜ì„¸ìš”.
            - ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±
            - êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ ì œê³µ
            - ì •ë‹µì´ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°, í•œ ê°œë§Œ ì„ íƒí•˜ê³  ì„ íƒ ì´ìœ ë¥¼ ê°„ëµíˆ í¬í•¨)
            '''},
        {"role": "assistant", "content": 'ì •ë‹µ:'}
    ]
    return messages

# GPU ë””ë°”ì´ìŠ¤ ì„¤ì • (0ë²ˆ GPUë¡œ ì„¤ì •)
device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")

# ëª¨ë¸ ê²½ë¡œ
model_path = '/SSL_NAS/concrete/models/models--meta-llama--Meta-Llama-3-8B-Instruct/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e5e23bbe8e749ef0efcf16cad411a7d23bd23298'

# ëª¨ë¸ ë¡œë“œ (low_cpu_mem_usage ì œê±°)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,   # GPU ë©”ëª¨ë¦¬ íš¨ìœ¨í™”
    device_map="auto"            # ìë™ìœ¼ë¡œ ìµœì ì˜ ì¥ì¹˜ì— ë¡œë“œ
)

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'left'

# 5. ì‹œë“œ ê³ ì • í•¨ìˆ˜
def set_seed(seed):
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

# 7. ê° ì¿¼ë¦¬ë§ˆë‹¤ Top-K ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±
k = 10  # top-k ìœ ì‚¬ ì§ˆë¬¸ ê°œìˆ˜
results = []
for q in tqdm(query):
    # 1. ì§ˆì˜ ì„ë² ë”©
    query_embedding = embed_texts([q])
    query_embedding = query_embedding.cpu().numpy().astype(np.float32)  
    faiss.normalize_L2(query_embedding)

    # 2. Top-K ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰
    distances, indices = index.search(query_embedding, k)
    top_10_embeddings = embeddings[indices[0]] 
    top_k_context = [data[i] for i in indices[0]]

    # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° ëª¨ë¸ ì…ë ¥
    messages = prompting_answer(q, top_k_context)
    templated_inputs = tokenizer.apply_chat_template(messages, tokenize=False)
    model_inputs = tokenizer(templated_inputs, padding=True, truncation=True, max_length=3500, return_tensors='pt').to(device)

    generated_ids = model.generate(**model_inputs, max_new_tokens=300)
    output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    answer = {q: output.split('Answer:')[-1].split('\n\n')[-1]}

    # 5. ê²°ê³¼ ì €ì¥
    result = {
        "query": q,
        "top_k": top_k_context,
        "answer": answer
    }
    results.append(result)

    # 6. ì½˜ì†” ì¶œë ¥
    print(f"\nğŸŸ¡ ì§ˆë¬¸: {q}")
    print("ğŸ”¹ Top-K ìœ ì‚¬ ì§ˆë¬¸:")
    for i, sim_q in enumerate(top_k_context, 1):
        print(f"   {i}. {sim_q}")
    print(f"âœ… ë‹µë³€: {answer}")

# 7. JSON íŒŒì¼ë¡œ ì €ì¥
with open("rag_query_results.json", "w", encoding="utf-8") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print("\nâœ… ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: rag_query_results.json")

# 8. GPU ìì› í•´ì œ
del model
del tokenizer
del embeddings
del model_inputs
torch.cuda.empty_cache()  # GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
gc.collect()              # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜